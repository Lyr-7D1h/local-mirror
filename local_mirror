#!/usr/bin/env node
//
// Fetch a website and save it locally
//

const puppeteer = require("puppeteer");
const fs = require("fs");
const path = require("path");
const minimist = require("minimist");
const crypto = require("crypto");

// Parse command line arguments
const argv = minimist(process.argv.slice(2));
const targetUrl = argv._[0];
if (typeof targetUrl !== "string") {
  console.error("Usage: download_page <url> [options]");
  process.exit(1);
}
const spanHosts = true;
const outputDir =
  argv.output || path.join(process.cwd(), new URL(targetUrl).hostname);
const maxDepth = argv.depth || 3;
const waitTime = argv.wait || 2;
const randomWait = argv.randomWait !== false;
const cookiesFile = argv.cookies || "cookies.txt";
const userAgent =
  argv.userAgent ||
  "Mozilla/5.0 (X11; Linux x86_64; rv:136.0) Gecko/20100101 Firefox/136.0";

// Track visited URLs and downloaded resources
const visitedUrls = new Set();
const downloadedResources = new Set();

// Ensure output directory exists
if (!fs.existsSync(outputDir)) {
  fs.mkdirSync(outputDir, { recursive: true });
}

// Load cookies from file
function loadCookies() {
  try {
    const cookiesContent = fs.readFileSync(cookiesFile, "utf8");
    // Basic parsing for Netscape cookie format
    return cookiesContent
      .split("\n")
      .filter((line) => line && !line.startsWith("#"))
      .map((line) => {
        const parts = line.split(/\t/);
        if (parts.length >= 7) {
          return {
            domain: parts[0],
            path: parts[2],
            secure: parts[3] === "TRUE",
            expires: parseInt(parts[4], 10),
            name: parts[5],
            value: parts[6],
          };
        }
        return null;
      })
      .filter(Boolean);
  } catch (error) {
    console.warn(
      `Could not load cookies from ${cookiesFile}: ${error.message}`
    );
  }
  return [];
}

// Delay function
function delay(time) {
  const actualTime = randomWait ? time * (0.5 + Math.random()) : time;
  console.log(`Delaying for ${actualTime} seconds`);
  return new Promise((resolve) => setTimeout(resolve, actualTime * 1000));
}

// Generate a filename from URL
function urlToFilename(pageUrl) {
  const parsedUrl = new URL(pageUrl);
  let filePath = parsedUrl.pathname;

  console.log(filePath);
  if (filePath.endsWith("/") || filePath === "") {
    filePath += "index.html";
  } else if (!path.extname(filePath)) {
    filePath += ".html";
  }

  const fullPath = path.join(outputDir, parsedUrl.hostname, filePath);
  const dirPath = path.dirname(fullPath);

  // Create directories if they don't exist
  if (!fs.existsSync(dirPath)) {
    fs.mkdirSync(dirPath, { recursive: true });
  }

  return fullPath;
}

// Save resource to file
async function saveResource(url, buffer, contentType) {
  try {
    // Create a hash of the URL to avoid filename issues
    const hash = crypto.createHash("md5").update(url).digest("hex");
    const parsedUrl = new URL(url);
    let filePath = parsedUrl.pathname;

    // Ensure file has appropriate extension
    if (!path.extname(filePath) && contentType) {
      const ext = contentType.split("/")[1]?.split(";")[0];
      if (ext) filePath += `.${ext}`;
    }

    const resourcePath = path.join(
      outputDir,
      "resources",
      hash.substring(0, 2),
      hash + path.extname(filePath)
    );
    const dirPath = path.dirname(resourcePath);

    if (!fs.existsSync(dirPath)) {
      fs.mkdirSync(dirPath, { recursive: true });
    }

    fs.writeFileSync(resourcePath, buffer);
    return path.relative(outputDir, resourcePath);
  } catch (error) {
    console.error(`Error saving resource ${url}: ${error.message}`);
    return null;
  }
}

// Convert links to work locally
function convertLinks(html, baseUrl, resourceMap) {
  // Replace resource URLs with local paths
  for (const [resourceUrl, localPath] of Object.entries(resourceMap)) {
    const escapedUrl = resourceUrl.replace(/[.*+?^${}()|[\]\\]/g, "\\$&");
    const regex = new RegExp(escapedUrl, "g");
    html = html.replace(regex, localPath);
  }
  return html;
}

// Main crawling function
async function crawlWebsite(pageUrl, depth = 0) {
  if (depth > maxDepth || visitedUrls.has(pageUrl)) {
    return;
  }

  visitedUrls.add(pageUrl);
  console.log(`Crawling [${depth}/${maxDepth}]: ${pageUrl}`);

  const browser = await puppeteer.launch({
    headless: "new",
  });

  // Set cookies
  const cookies = loadCookies();
  if (cookies.length > 0) {
    await browser.setCookie(...cookies);
  }

  try {
    const page = await browser.newPage();

    // Set user agent
    await page.setUserAgent(userAgent);

    // Enable request interception
    await page.setRequestInterception(true);

    const resourceMap = {};

    page.on("request", async (request) => {
      request.continue();
    });

    page.on("response", async (response) => {
      const responseUrl = response.url();

      if (downloadedResources.has(responseUrl)) {
        return;
      }

      // Skip non-successful responses
      if (!response.ok()) {
        return;
      }

      try {
        const contentType = response.headers()["content-type"] || "";

        // Skip HTML documents (we'll save them separately)
        if (contentType.includes("text/html")) {
          return;
        }

        const buffer = await response.buffer().catch(() => null);
        if (!buffer) return;

        downloadedResources.add(responseUrl);
        const localPath = await saveResource(responseUrl, buffer, contentType);
        if (localPath) resourceMap[responseUrl] = localPath;
      } catch (error) {
        console.error(`Error downloading ${responseUrl}: ${error.message}`);
      }
    });

    // Navigate to the page and wait for load
    await page.goto(pageUrl, {
      waitUntil: "networkidle0",
      timeout: 60000,
    });

    // Wait for any dynamic content
    await delay(2);

    // Get the page content
    let content = await page.content();

    // Save the page
    const filePath = urlToFilename(pageUrl);
    content = convertLinks(content, pageUrl, resourceMap);
    fs.writeFileSync(filePath, content);

    // Extract links for further crawling
    const links = await page.evaluate(() => {
      return Array.from(document.querySelectorAll("a[href]"))
        .map((a) => a.href)
        .filter(
          (href) =>
            href && !href.startsWith("javascript:") && !href.startsWith("#")
        );
    });

    // Close the browser
    await browser.close();

    // Wait between requests
    await delay(waitTime);

    // Recursively crawl linked pages
    console.log(`Found ${links.length} links`);
    const baseUrl = new URL(pageUrl);
    for (const link of links) {
      try {
        const linkUrl = new URL(link);

        // Apply no-parent rule - skip if link goes to parent directory
        if (
          linkUrl.hostname === baseUrl.hostname &&
          !linkUrl.pathname.startsWith(baseUrl.pathname)
        ) {
          continue;
        }

        await crawlWebsite(link, depth + 1);
      } catch (error) {
        // Skip invalid URLs
      }
    }
  } catch (error) {
    console.error(`Error crawling ${pageUrl}: ${error.message}`);
    await browser.close();
  }
}

console.log(`Starting download of ${targetUrl} to ${outputDir}`);
crawlWebsite(targetUrl).catch(console.error);
